{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZixPwrj87XF",
        "outputId": "8d04e124-5aba-4cb4-a628-832c4e8240a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rake_nltk in c:\\users\\shyam\\anaconda3\\lib\\site-packages (1.0.6)\n",
            "Requirement already satisfied: wordcloud in c:\\users\\shyam\\anaconda3\\lib\\site-packages (1.9.2)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\shyam\\anaconda3\\lib\\site-packages (3.7.1)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\shyam\\anaconda3\\lib\\site-packages (from matplotlib) (5.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\shyam\\anaconda3\\lib\\site-packages (from matplotlib) (20.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\shyam\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\shyam\\anaconda3\\lib\\site-packages (from matplotlib) (8.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\shyam\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.20 in c:\\users\\shyam\\anaconda3\\lib\\site-packages (from matplotlib) (1.24.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\shyam\\anaconda3\\lib\\site-packages (from matplotlib) (1.0.7)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\shyam\\anaconda3\\lib\\site-packages (from matplotlib) (4.39.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\shyam\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\shyam\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: six in c:\\users\\shyam\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\shyam\\anaconda3\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.4.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in c:\\users\\shyam\\anaconda3\\lib\\site-packages (from rake_nltk) (3.8.1)\n",
            "Requirement already satisfied: joblib in c:\\users\\shyam\\anaconda3\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (1.0.1)\n",
            "Requirement already satisfied: click in c:\\users\\shyam\\anaconda3\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\shyam\\anaconda3\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\shyam\\anaconda3\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (4.59.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install rake_nltk wordcloud matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "n_LDvF9B8ZiS"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from rake_nltk import Rake\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY4h2ERU_COx",
        "outputId": "beea967f-22d7-4bf5-90a3-27daebb930b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\shyam\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\shyam\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\shyam\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download the resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "f8k_0IfI8gX1"
      },
      "outputs": [],
      "source": [
        "def wikiscrape(url):\n",
        "    # Get URL\n",
        "    page = requests.get(url)\n",
        "\n",
        "    # Scrape webpage\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "    # Find all occurrences of p in HTML\n",
        "    # Includes HTML tags\n",
        "    paragraphs = soup.find_all('p')\n",
        "\n",
        "    # Remove square brackets and their contents using regular expressions\n",
        "    pattern = re.compile(r'\\[[^\\]]*\\]')\n",
        "    content = \"\"\n",
        "    for p in paragraphs:\n",
        "        cleaned_text = re.sub(pattern, '', p.get_text())\n",
        "        content += cleaned_text.lower()\n",
        "\n",
        "    return content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ryfdcs1k8iq9"
      },
      "outputs": [],
      "source": [
        "def lemmatize_and_get_forms(word):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    forms = set()\n",
        "    forms.add(lemmatizer.lemmatize(word))\n",
        "    for synset in nltk.corpus.wordnet.synsets(word):\n",
        "        for lemma in synset.lemmas():\n",
        "            forms.add(lemma.name())\n",
        "    return forms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xcQxwpl_8qFN"
      },
      "outputs": [],
      "source": [
        "def rake(text, ignore_words, keep_words):\n",
        "    # Remove punctuation except hyphen\n",
        "    translator = str.maketrans(\"\", \"\", string.punctuation.replace(\"-\", \"\"))\n",
        "    text = text.translate(translator)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "    # Reconstruct the modified text\n",
        "    modified_text = \" \".join(tokens)\n",
        "\n",
        "    # Initialize RAKE\n",
        "    r = Rake()\n",
        "\n",
        "    # Extract keywords using RAKE\n",
        "    r.extract_keywords_from_text(modified_text)\n",
        "\n",
        "    # Get the ranked keyword phrases\n",
        "    ranked_keywords = r.get_ranked_phrases()\n",
        "\n",
        "    # Filter out keywords that contain ignore words\n",
        "    filtered_keywords = [keyword for keyword in ranked_keywords if not any(word in keyword for word in ignore_words)]\n",
        "\n",
        "    # Filter out keywords that do not contain any of the keep words or their forms\n",
        "    if keep_words:\n",
        "        final_forms = set()\n",
        "        for word in keep_words:\n",
        "            word_forms = lemmatize_and_get_forms(word)\n",
        "            print(f\"Word: {word} ; Word-forms: {word_forms}\")\n",
        "            final_forms.update(word_forms)\n",
        "            filtered_keywords = [keyword for keyword in filtered_keywords if any(word in keyword for word in final_forms)]\n",
        "\n",
        "        print(f\"\\nAll Word Forms: {final_forms}\\n\")\n",
        "\n",
        "\n",
        "    return filtered_keywords[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "H-qRuT-BGEiw"
      },
      "outputs": [],
      "source": [
        "def generate_word_cloud(keywords):\n",
        "    # Join the keywords into a single string\n",
        "    text = \" \".join(keywords)\n",
        "\n",
        "    # Generate the word cloud\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(text)\n",
        "\n",
        "    # Display the word cloud\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "CHbak0B2BvXl",
        "outputId": "c1899731-6fd6-4e58-dcc6-db316c0e9b7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'def rake(text, ignore_words, keep_words):\\n    # Remove punctuation except hyphen\\n    translator = str.maketrans(\"\", \"\", string.punctuation.replace(\"-\", \"\"))\\n    text = text.translate(translator)\\n\\n    # Tokenize the text\\n    tokens = word_tokenize(text)\\n\\n    # Remove stopwords\\n    stop_words = set(stopwords.words(\\'english\\'))\\n    tokens = [token for token in tokens if token.lower() not in stop_words]\\n\\n    # Reconstruct the modified text\\n    modified_text = \" \".join(tokens)\\n\\n    # Initialize RAKE\\n    r = Rake()\\n\\n    # Extract keywords using RAKE\\n    r.extract_keywords_from_text(modified_text)\\n\\n    # Get the ranked keyword phrases\\n    ranked_keywords = r.get_ranked_phrases()\\n\\n    # Filter out keywords that contain ignore words\\n    filtered_keywords = [keyword for keyword in ranked_keywords if not any(word in keyword for word in ignore_words)]\\n\\n    # Filter out keywords that do not contain any of the keep words or their forms\\n    if keep_words:\\n        final_forms = set()\\n        for word in keep_words:\\n            word_forms = lemmatize_and_get_forms(word)\\n            print(f\"Word: {word} ; Word-forms: {word_forms}\")\\n            final_forms.update(word_forms)\\n\\n        print(f\"\\nAll Word Forms: {final_forms}\\n\")\\n\\n    if final_forms:\\n        filtered_keywords = [keyword for keyword in filtered_keywords if any(word in keyword for word in final_forms)]\\n\\n    return filtered_keywords[:20]'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"def rake(text, ignore_words, keep_words):\n",
        "    # Remove punctuation except hyphen\n",
        "    translator = str.maketrans(\"\", \"\", string.punctuation.replace(\"-\", \"\"))\n",
        "    text = text.translate(translator)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "    # Reconstruct the modified text\n",
        "    modified_text = \" \".join(tokens)\n",
        "\n",
        "    # Initialize RAKE\n",
        "    r = Rake()\n",
        "\n",
        "    # Extract keywords using RAKE\n",
        "    r.extract_keywords_from_text(modified_text)\n",
        "\n",
        "    # Get the ranked keyword phrases\n",
        "    ranked_keywords = r.get_ranked_phrases()\n",
        "\n",
        "    # Filter out keywords that contain ignore words\n",
        "    filtered_keywords = [keyword for keyword in ranked_keywords if not any(word in keyword for word in ignore_words)]\n",
        "\n",
        "    # Filter out keywords that do not contain any of the keep words or their forms\n",
        "    if keep_words:\n",
        "        final_forms = set()\n",
        "        for word in keep_words:\n",
        "            word_forms = lemmatize_and_get_forms(word)\n",
        "            print(f\"Word: {word} ; Word-forms: {word_forms}\")\n",
        "            final_forms.update(word_forms)\n",
        "\n",
        "        print(f\"\\nAll Word Forms: {final_forms}\\n\")\n",
        "\n",
        "    if final_forms:\n",
        "        filtered_keywords = [keyword for keyword in filtered_keywords if any(word in keyword for word in final_forms)]\n",
        "\n",
        "    return filtered_keywords[:20]\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "id": "_xRx5K_28riV",
        "outputId": "3259d682-9866-4b64-b2eb-913f1fa2ec35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered Keywords:\n",
            "1. time high vast majority resources spent interdiction law enforcement instead public health united states number nonviolent drug offenders prison exceeds 100000 total incarcerated population eu despite fact eu 100 million citizens despite drug legislation perhaps large organized criminal drug cartels operate worldwide advocates decriminalization argue drug prohibition makes drug dealing lucrative business leading much associated criminal activity states us late focused facilitating safe use opposed eradicating example 2022 new jersey made effort expand needle exchange programs throughout state passing bill legislature gives control decisions regarding types programs states department health state level bill significant new jersey could used model states possibly follow well bill partly reaction issues occurring local level city governments within state new jersey late one example atlantic city government came lawsuit halted enactment said programs within city suit came year passing bill stemming local level decision shut related operations atlantic city made july year lawsuit highlights feelings new jersey residents great influence bill passing legislature feelings demonstrated front atlantic city city hall residents exclaimed desire programs aforementioned bill signed effectively law days passed legislature new jersey governor phil murphy policymakers try understand relative costs drug\n",
            "2. related public expenditure national level reported 10 countries categorized according functions government amounting total eur 217 billion overall highest proportion total came within government functions health 66 eg medical services public order safety pos 20 eg police services law courts prisons country average share gdp 0023 health 0013 pos however shares varied considerably across countries ranging 000033 slovakia 0053 gdp ireland case health 0003 portugal 002 uk case pos almost 161\n",
            "3. related public expenditure showed positive association gdp across countries considered r 081 case health r 091 pos percentage change health pos expenditures due one percent increase gdp income elasticity demand estimated 178 123 respectively highly income elastic health pos expenditures considered luxury goods nation becomes wealthier openly spends proportionately drug\n",
            "4. related issues product two give rough estimate compared across different countries part reporting exercise corresponding 2005 european monitoring centre drugs drug addictions network national focal points set 27 european union eu member states norway candidates countries eu requested identify labeled drug\n",
            "5. based treatment group cbt treatments administered variety different formats varying levels research support research shown makes group cbt effective promotes development social skills developmentally appropriate emotional regulatory skills interpersonal skills integrated treatment models combines parts various types treatment also seen well\n",
            "6. related deaths since 2000 mostly related heroin use switzerland seen large decreases 2018 approximately 60000 people died drug overdoses america time period switzerlands drug deaths 260 relative population countries us 10 times drug\n",
            "7. established probably effective study maternal alcohol drug use shown integrated treatment programs produced significant results resulting higher negative results toxicology screens additionally brief school\n",
            "8. related deaths compared swiss confederation effect illustrates efficacy hat reduce fatal outcomes opiateopioid addiction common individuals drugs use disorder psychological problems terms dual diagnosis co\n",
            "9. related public expenditure across countries study compared health pos spending gdp 10 reporting countries results suggest gdp major determinant health pos drug\n",
            "10. related expenditures defined direct planned spending reflects voluntary engagement state field illicit drugs direct public expenditures explicitly labeled drug\n",
            "11. related expenditures given country two elements would necessary overall prison expenditures country given period attributable proportion inmates due drug\n",
            "12. also provide link support structure street family peer group potent symbol shared experience order maintain high\n",
            "13. related deaths achieved countries example us devoid measures seen large increases drug\n",
            "14. related public expenditure based classification system costs properly identified labelled drug\n",
            "15. boyce argues drug use amongst street children primarily functional — dulling senses hardships life street —\n",
            "16. reporting questionnaire found 178 homeless veteran participants attributed homelessness alcohol drug\n",
            "17. related expenditure overall expenditure × attributable proportion example estimate prison drug\n",
            "18. social behavior occurs person influence drug long\n",
            "19. related interventions appropriate drug policy relies assessment drug\n",
            "20. end reports unlabelled expenditure refers unplanned spending estimated modeling techniques based top\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'TransposedFont' object has no attribute 'getbbox'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-18-1ac98bc95b7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# Generate and display the word cloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mgenerate_word_cloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_keywords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-16-d35ab55d215d>\u001b[0m in \u001b[0;36mgenerate_word_cloud\u001b[1;34m(keywords)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Generate the word cloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mwordcloud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m800\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackground_color\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"white\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Display the word cloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\shyam\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    637\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m         \"\"\"\n\u001b[1;32m--> 639\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\shyam\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    619\u001b[0m         \"\"\"\n\u001b[0;32m    620\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 621\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    622\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\shyam\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    451\u001b[0m                 \u001b[0mfont_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m                 self.generate_from_frequencies(dict(frequencies[:2]),\n\u001b[0m\u001b[0;32m    454\u001b[0m                                                max_font_size=self.height)\n\u001b[0;32m    455\u001b[0m                 \u001b[1;31m# find font sizes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\shyam\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    506\u001b[0m                     font, orientation=orientation)\n\u001b[0;32m    507\u001b[0m                 \u001b[1;31m# get size of resulting text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m                 \u001b[0mbox_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextbbox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfont\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransposed_font\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"lt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    509\u001b[0m                 \u001b[1;31m# find possible places using integral image:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m                 result = occupancy.sample_position(box_size[3] + self.margin,\n",
            "\u001b[1;32mc:\\Users\\shyam\\anaconda3\\lib\\site-packages\\PIL\\ImageDraw.py\u001b[0m in \u001b[0;36mtextbbox\u001b[1;34m(self, xy, text, font, anchor, spacing, align, direction, features, language, stroke_width, embedded_color)\u001b[0m\n\u001b[0;32m    655\u001b[0m             \u001b[0mfont\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetfont\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    656\u001b[0m         \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"RGBA\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0membedded_color\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfontmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 657\u001b[1;33m         bbox = font.getbbox(\n\u001b[0m\u001b[0;32m    658\u001b[0m             \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstroke_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m         )\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'TransposedFont' object has no attribute 'getbbox'"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    # Input URL\n",
        "    last_part = input(\"Enter the last part of the Wikipedia URL (e.g., 'hotflashes'): \")\n",
        "    if last_part == '1':\n",
        "        break\n",
        "\n",
        "    url = \"https://en.wikipedia.org/wiki/\" + last_part\n",
        "\n",
        "    # Customized stopwords specific to your problem\n",
        "    custom_stopwords = [\"a\", \"an\", \"the\"]\n",
        "\n",
        "    # Words to ignore in the final keyword list\n",
        "    ignore_words_input = input(\"Enter the words to ignore in the final keyword list, separated by hyphens: \")\n",
        "    ignore_words = ignore_words_input.split(\"-\") if ignore_words_input != '1' else []\n",
        "\n",
        "    # Words to keep in the final keyword list\n",
        "    keep_words_input = input(\"Enter the words to keep in the final keyword list, separated by hyphens: \")\n",
        "    keep_words = keep_words_input.split(\"-\") if keep_words_input != '1' else []\n",
        "\n",
        "    # Calling wikiscrape()\n",
        "    article_text = wikiscrape(url)\n",
        "\n",
        "    # Apply RAKE algorithm\n",
        "    filtered_keywords = rake(article_text, ignore_words, keep_words)\n",
        "\n",
        "    # Print the filtered keywords\n",
        "    if filtered_keywords:\n",
        "        print(\"Filtered Keywords:\")\n",
        "        count = 1\n",
        "        for keyword in filtered_keywords:\n",
        "            print(f\"{count}. {keyword}\")\n",
        "            count += 1\n",
        "    else:\n",
        "        print(\"No keywords found.\")\n",
        "\n",
        "    # Generate and display the word cloud\n",
        "    generate_word_cloud(filtered_keywords)\n",
        "\n",
        "    print()\n",
        "\n",
        "print(\"Exiting the program.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
